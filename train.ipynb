{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1cKcsHNCabuSo2jafgVdtFDpMC4s4TD5_","authorship_tag":"ABX9TyNFUJi/ImRWbklqqRnsmnPP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zw20inQMwwM8"},"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Retinal_Vessel_Segmentation')\n","sys.path.append('/content/drive/MyDrive/Retinal_Vessel_Segmentation/libraries')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvNhRD000KIs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620006087999,"user_tz":-330,"elapsed":4097,"user":{"displayName":"SAMBIT TARAI ed16b026","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn7TwWSpIdSSmrkS7IaP7R0EYJ2dk9bT3iF5ROOw=s64","userId":"16852229471097488939"}},"outputId":"91150ed5-a27d-4cc3-ef8d-3d101b761f3a"},"source":["pip install tensorboardx"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorboardx\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n","\r\u001b[K     |██▊                             | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 26.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 28.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 31.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 29.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 31.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 20.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 18.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 20.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 19.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 19.7MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.19.5)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (56.0.0)\n","Installing collected packages: tensorboardx\n","Successfully installed tensorboardx-2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KjSpSWbS0MO4"},"source":["import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","from tqdm import tqdm\n","import random,sys,time\n","import os\n","from os.path import join\n","import torch\n","from libraries.extract_patches import get_data_train\n","from libraries.losses.loss import *\n","from libraries.visualize import group_images, save_img\n","from libraries.common import *\n","from libraries.dataset import TrainDataset,TestDataset\n","from torch.utils.data import DataLoader\n","from config import parse_args\n","from libraries.logger import Logger, Print_Logger\n","from collections import OrderedDict\n","from libraries.metrics import Evaluate\n","import models\n","#from models import UNetFamily\n","#from test import Test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JT9C8Mmm0Pkr"},"source":["#  Load the data and extract patches\n","def get_dataloader(val_ratio, batch_size):\n","    # patches_imgs_train, patches_masks_train = get_data_train(\n","    #     data_path_list = args.train_data_path_list,\n","    #     patch_height = args.train_patch_height,\n","    #     patch_width = args.train_patch_width,\n","    #     N_patches = args.N_patches,\n","    #     inside_FOV = args.inside_FOV #select the patches only inside the FOV  (default == False)\n","    # )\n","\n","    patches_imgs_train = np.load('/content/drive/MyDrive/Retinal_Vessel_Segmentation/patches_imgs_train.npy')\n","    patches_masks_train = np.load('/content/drive/MyDrive/Retinal_Vessel_Segmentation/patches_masks_train.npy')\n","\n","    val_ind = random.sample(range(patches_masks_train.shape[0]),int(np.floor(val_ratio*patches_masks_train.shape[0])))\n","    train_ind =  set(range(patches_masks_train.shape[0])) - set(val_ind)\n","    train_ind = list(train_ind)\n","\n","    train_set = TrainDataset(patches_imgs_train[train_ind,...],patches_masks_train[train_ind,...],mode=\"train\")\n","    train_loader = DataLoader(train_set, batch_size=batch_size,\n","                              shuffle=True, num_workers=4)\n","\n","    val_set = TrainDataset(patches_imgs_train[val_ind,...],patches_masks_train[val_ind,...],mode=\"val\")\n","    val_loader = DataLoader(val_set, batch_size=batch_size,\n","                            shuffle=False, num_workers=4)\n","    \n","    # Save some samples of feeding to the neural network\n","    # N_sample = min(patches_imgs_train.shape[0], 50)\n","    # save_img(group_images((patches_imgs_train[0:N_sample, :, :, :]*255).astype(np.uint8), 10),\n","    #           join(args.outf, args.save, \"sample_input_imgs.png\"))\n","    # save_img(group_images((patches_masks_train[0:N_sample, :, :, :]*255).astype(np.uint8), 10),\n","    #           join(args.outf, args.save,\"sample_input_masks.png\"))\n","\n","    return train_loader,val_loader\n","\n","# train \n","def train(train_loader,net,criterion,optimizer,device):\n","    net.train()\n","    train_loss = AverageMeter()\n","\n","    for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader), total=len(train_loader)):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss.update(loss.item(), inputs.size(0))\n","    log = OrderedDict([('train_loss',train_loss.avg)])\n","    return log\n","\n","# val \n","def val(val_loader,net,criterion,device):\n","    net.eval()\n","    val_loss = AverageMeter()\n","    evaluater = Evaluate()\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in tqdm(enumerate(val_loader), total=len(val_loader)):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","            val_loss.update(loss.item(), inputs.size(0))\n","\n","            outputs = outputs.data.cpu().numpy()\n","            targets = targets.data.cpu().numpy()\n","            evaluater.add_batch(targets,outputs[:,1])\n","    log = OrderedDict([('val_loss', val_loss.avg), \n","                       ('val_acc', evaluater.confusion_matrix()[1]), \n","                       ('val_f1', evaluater.f1_score()),\n","                       ('val_auc_roc', evaluater.auc_roc())])\n","    return log"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p8EfZRE65_eo","executionInfo":{"status":"ok","timestamp":1619941764051,"user_tz":-330,"elapsed":24902,"user":{"displayName":"SAMBIT TARAI ed16b026","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn7TwWSpIdSSmrkS7IaP7R0EYJ2dk9bT3iF5ROOw=s64","userId":"16852229471097488939"}},"outputId":"3367ed92-d025-469e-fdc4-a69f09abef67"},"source":["setpu_seed(2021)\n","outf = \"/content/drive/MyDrive/Retinal_Vessel_Segmentation/Experiments\"\n","save = \"UNet_vessel_seg\"\n","save_path = os.path.join(outf, save)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() and True else \"cpu\")\n","cudnn.benchmark = True\n","\n","log = Logger(save_path)\n","sys.stdout = Print_Logger(os.path.join(save_path,'train_log.txt'))\n","print('The computing device used is: ','GPU' if device.type=='cuda' else 'CPU')\n","\n","net = models.UNetFamily.U_Net(1,2).to(device)\n","print(\"Total number of parameters: \" + str(count_parameters(net)))\n","log.save_graph(net,torch.randn((1,1,64,64)).to(device).to(device=device))\n","\n","N_epochs = 3\n","start_epoch = 1\n","criterion = CrossEntropyLoss2d() # Initialize loss function\n","optimizer = optim.Adam(net.parameters(), lr=0.0005)\n","lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N_epochs, eta_min=0)\n","val_ratio = 0.1\n","batch_size = 64\n","\n","train_loader, val_loader = get_dataloader(val_ratio, batch_size) # create dataloader"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The computing device used is:  GPU\n","Total number of parameters: 34525954\n","Architecture of Model have saved in Tensorboard!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEvR5H9t6_lH","executionInfo":{"status":"ok","timestamp":1619935259449,"user_tz":-330,"elapsed":318243,"user":{"displayName":"SAMBIT TARAI ed16b026","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn7TwWSpIdSSmrkS7IaP7R0EYJ2dk9bT3iF5ROOw=s64","userId":"16852229471097488939"}},"outputId":"f9c0f87e-ef47-4b8d-90d7-2ffae041bc62"},"source":["best = {'epoch':0,'AUC_roc':0.5} # Initialize the best epoch and performance(AUC of ROC)\n","trigger = 0  # Early stop Counter\n","\n","for epoch in range(start_epoch,N_epochs+1):\n","  print('\\nEPOCH: %d/%d --(learn_rate:%.6f) | Time: %s' % \\\n","            (epoch, N_epochs,optimizer.state_dict()['param_groups'][0]['lr'], time.asctime()))\n","  \n","  # train stage\n","  train_log = train(train_loader,net,criterion, optimizer,device)\n","  # val stage\n","  val_log = val(val_loader,net,criterion,device)\n","\n","  log.update(epoch,train_log,val_log) # Add log information\n","  lr_scheduler.step()\n","\n","  # Save checkpoint of latest and best model\n","  state = {'net': net.state_dict(),'optimizer':optimizer.state_dict(),'epoch': epoch}\n","  torch.save(state, join(save_path, 'latest_model.pth'))\n","  trigger += 1\n","  if val_log['val_auc_roc'] > best['AUC_roc']:\n","    print('\\033[0;33mSaving best model!\\033[0m')\n","    torch.save(state, join(save_path, 'best_model.pth'))\n","    best['epoch'] = epoch\n","    best['AUC_roc'] = val_log['val_auc_roc']\n","    trigger = 0\n","\n","  print('Best performance at Epoch: {} | AUC_roc: {}'.format(best['epoch'],best['AUC_roc']))\n","\n","  # early stopping\n","  early_stop = 6\n","  if not early_stop is None:\n","    if trigger >= early_stop:\n","      print(\"=> early stopping\")\n","      break\n","\n","  torch.cuda.empty_cache()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","100%|██████████| 282/282 [01:16<00:00,  3.67it/s]\n","100%|██████████| 32/32 [00:03<00:00, 10.00it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","100%|██████████| 282/282 [01:15<00:00,  3.71it/s]\n","100%|██████████| 32/32 [00:03<00:00, 10.02it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","100%|██████████| 282/282 [01:16<00:00,  3.70it/s]\n","100%|██████████| 32/32 [00:03<00:00, 10.08it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4ZCDgN_b3gi2"},"source":[""],"execution_count":null,"outputs":[]}]}